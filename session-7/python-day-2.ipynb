{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 File input and output\n",
    "One of the most important tools in Python is the ability to read files from your computer and write new files or update existing files. I am going to go through a few simple examples of how to read and write data using Python.\n",
    "## 1.1 Reading a file\n",
    "Reading a file requires the opening of a file handle and then use of one of the several file read methods:\n",
    "### 1.1.1 The read() method\n",
    "This method will read the entire contents of the filehandle into a variable. You can limit the amount read by feeding the method a number as an argument. Consider the following example, which will read the first 11 bytes from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar=open('company20194.idx','r')\n",
    "print(edgar.read(1000))\n",
    "edgar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 The readline() method\n",
    "You can read also read entire lines of the file using **readline()**. Each time the method is invoked, it will return a subsequent line from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar=open('company20194.idx','r')\n",
    "print(\"Line 1: \", edgar.readline())\n",
    "print(\"Line 2: \", edgar.readline())\n",
    "print(\"Line 3: \", edgar.readline())\n",
    "edgar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 The readlines() method\n",
    "You can also read the lines of the file into a list such that each element of the list contains a different line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar=open('company20194.idx','r')\n",
    "x=edgar.readlines()\n",
    "print(len(x))\n",
    "for line in range(10):\n",
    "    print(x[line])\n",
    "edgar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Reading lines with a for loop\n",
    "You can also read lines one-by-one withe a **for** loop. Notice the **enumerate** function that is used in the loop declaration. This is a convenient way to create a line counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar=open('company20194.idx','r')\n",
    "for (c, line) in enumerate(edgar,1):\n",
    "    if c<11:\n",
    "        print(line)\n",
    "edgar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Using the with statement to avoid the need to close filehandles\n",
    "When you open a filehandle, it needs to be closed. This can be somewhat annoying. A covenient workaround is the use of the **with** statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('company20194.idx','r') as edgar:\n",
    "    for (c, line) in enumerate(edgar,1):\n",
    "        if c<11:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Writing to a file\n",
    "Writing text to a file is very similar to reading a file. One difference is that when you open the filehandle for writing, you will need to specify either 'w' or 'a' instead of 'r' depending on whether you would like to overwrite an existing file or append to a file if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as fileout:\n",
    "    fileout.write('This is how we write to a file.\\n')\n",
    "    \n",
    "with open('output.txt', 'a') as f2:\n",
    "    f2.write('Now we are appending another line to the file.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Getting WRDS data with Python\n",
    "Since the WRDS Cloud now houses its datasets in PostgreSQL format rather than a pile of .sas7bdat files, it is very convenient to pull some data from WRDS for either analysis or as part of some other task that you might have in mind. We will spend just a few minutes with the **wrds** module before moving into some more details about **pandas** and ways of looking at numerical data within Python. I encourage you to play around with Python and data analysis. See if you can use it for any tasks that might have been done in SAS or Stata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "db = wrds.Connection(wrds_username='sbonsall')\n",
    "\n",
    "\n",
    "#The step below creates a file on your computer so that you do not \n",
    "#need to enter your password in the future when using the WRDS\n",
    "#module. Quite convenient!\n",
    "\n",
    "#db.create_pgpass_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the WRDS libraries\n",
    "db.list_libraries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the tables within a \n",
    "db.list_tables('rpna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also describe the contents of a table\n",
    "\n",
    "db.describe_table(library='comp', table='adsprate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are two ways of querying data from WRDS. \n",
    "#The first is get_table and the second is raw_sql.\n",
    "\n",
    "## get_table\n",
    "\n",
    "ratings=db.get_table(library='comp',table='adsprate', columns=['gvkey','datadate','splticrm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[ratings['gvkey']=='001690']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## raw_sql\n",
    "\n",
    "prices=db.raw_sql(\"select permno, date, ret from crsp.dsf where date_part('year',date)=2019 limit 100\")\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices2=db.raw_sql(\"select permno, date, ret from crsp.dsf where date_part('year',date)=2019\")  \n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Working with tabular data: The Pandas package\n",
    "Text analysis typically involves unstructured and structured data. Un- structured data is data that is not organized in a pre-defined manner, such as a 10-K or an image. A 10-K is unstructured since each company organizes its 10-K differently. Much of this paper deals with unstruc- tured text data. However, researchers performing text analysis will also work with structured data such as SAS and Stata datasets, Excel files, and CSV files corresponding to Compustat, IBES, and CRSP data. Structured data is organized into pre-defined columns, each of which contains a specific type of data, and rows, each of which contains an observation.\n",
    "## 3.1 Required import statements\n",
    "The most common method of importing the Pandas library is with this call to the **import** statement. It is common practice to use the alias pd for Pandas, and we recommend the reader to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas library is built on top of a numerical analysis library called NumPy, and most Pandas users frequently call NumPy functions to manipulate data. Therefore, most users include the following import statements at the top of their notebooks. We recommend the reader to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Importing and exporting data\n",
    "In this section, we introduce Pandas’ capabilities for importing data from Excel, CSV, Stata, and SAS formats. We provide sample code and explanation for common use cases. Pandas provides powerful and flexible import capabilities and supports many other data formats. Consult the [official documentation](https://pandas.pydata.org/docs/user_guide/io.html) for use cases not covered here.\n",
    "\n",
    "All Pandas functions for importing data begin with **read_**, e.g. **pd.read_excel**, **pd.read_sas**, etc. The options for these functions are fairly standardized so the learning curve flattens after the user gains proficiency with one import function.\n",
    "### 3.2.1 Importing data from Excel\n",
    "Use the function **pd.read_excel** to import data from an Excel file. If the Excel file contains only one worksheet with well-formatted data (i.e., one contiguous block of data with header row and no blank lines at the top of the worksheet) then **pd.read_excel** requires only one argument, the path to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_excel('Categorical_EPU_Data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pd.read_excel function assumed that the first row contained column names and automatically used them as column labels in the DataFrame. You can preview the DataFram by typing its name (**df**) into a code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One word of caution: attempting to use **pd.read_excel** on a file that is open in Excel will throw an error. To be able to read the file in Pandas, we need to close the file in Excel and try again.\n",
    "#### Using pd.read_excel with an Excel Workbook Containing Multiple Worksheets\n",
    "Excel workbooks often contain multiple worksheets. By default, the function **pd.read_excel** will read data from the first worksheet in the workbook. To read a different worksheet, use the optional keyword argument sheet_name. This argument accepts the following values:\n",
    "- A case-sensitive string containing the name of the worksheet.\n",
    "- A zero-based integer index of the worksheet. 0 means the first\n",
    "worksheet, 1 means the second worksheet, and so on.\n",
    "\n",
    "#### Using pd.read_excel When the Header Row is Missing\n",
    "By default, **read_excel** assumes that the first row of data contains column **names**. If these column names are missing, pass the keyword argument **header=None**. Provide column names by passing a list to the optional keyword argument names.\n",
    "#### Skipping Rows and Blank Lines\n",
    "Many Excel users place blank lines and text above the data in a worksheet. To handle this use case, use the optional keyword argument skiprows when calling **pd.read_excel**. To skip the first n lines of the file, use **skiprows=n**. Alternatively, to skip specific rows, pass a list of row numbers; unlike Excel, skiprows assumes row numbers begin at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_excel('Categorical_EPU_Data.xlsx', header=None, skiprows=1, names=['date','epu','monetary_policy','fiscal_policy','taxes','gov_spending','health_care','national_security','entitlement_programs','regulation','financial_regulation','trade_policy','sov_debt_curr_crises'])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Help for pd.read_excel\n",
    "The official documentation page for pd.read_excel is located here. Alternatively, type **help(pd.read_excel)** into a Jupyter notebook cell and execute the cell.\n",
    "### 3.2.2 Importing data from a CSV file\n",
    "CSV stands for “comma separated value.” A CSV file is a text file that uses commas to separate, or delimit, values. CSV format is commonly used to store small-to-medium sized data files because it can be used to transfer data between different software (e.g., SAS and Stata), and because all major operating systems support it.\n",
    "\n",
    "By convention, the first row of a CSV file contains the column names. The Pandas function **pd.read_csv** is similar to **pd.read_excel**. The two functions accept many of the same arguments, such as **skiprows** and **names**. However, since CSV files are a text files. Some issues commonly arise when importing financial data in CSV format. In the remainder of this section, we demonstrate how to handle two of these issues: parsing date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df3=pd.read_csv('EarningsNews.csv')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Dates\n",
    "The **pd.read_csv** function can parse dates. Simply tell the function which columns contain dates through the parse_dates keyword ar- gument, and Pandas usually imports the dates correctly.\n",
    "\n",
    "To import the *date* field from our CSV file, we can use the following modification to our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.read_csv('EarningsNews.csv', parse_dates=['date'], dayfirst=True)\n",
    "df4.info()\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Help for pd.read_csv\n",
    "The official documentation page for **pd.read_csv** is located [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). Alternatively, type **help(pd.read_csv)** into a Jupyter notebook cell and execute the cell.\n",
    "### 3.2.3 Importing Data from Stata\n",
    "Use the function **pd.read_stata** to read Stata .dta files. At the time of this writing, **pd.read_stata** supports Stata versions 10–14. If your Stata file was written by a later version of Stata, either save it to an earlier version or save it to Excel or CSV.\n",
    "### 3.2.4 Importing Data from SAS\n",
    "Use the function **pd.read_sas** to read SAS files. This function can read SAS xport (.XPT) files and SAS7BDAT files. In our experience, this function can be finicky. If you have a valid SAS license and SAS installation on your computer and plan to regularly pass data between SAS and Python, we highly recommend the package **SASPy**. This package was written by The SAS Institute and is officially supported.\n",
    "### 3.2.5 Exporting Data\n",
    "Every **read_** function discussed in this paper has a corresponding **write_** function. It is therefore straightforward to save data to a desired format.\n",
    "## 3.3 Viewing Data in Pandas\n",
    "Researchers working with Pandas in Jupyter Notebooks have several options for viewing data. To view an entire DataFrame (or Series), simply type the name of the variable into a cell and run the cell. The data will be shown underneath the cell. If the DataFrame has many rows, Jupyter will show the top and bottom rows. Pandas users typically only need to view a few rows at a time. This is accomplished using the head and tail commands. For example, if the variable df stores a DataFrame, **df.head()** will display the first 5 rows of that DataFrame, and **df.tail()** will display the last 5 rows. To specify the number of rows to display, pass an optional argument to head or tail. Thus, **df.head(3)** will display the first 3 rows of *df*.\n",
    "\n",
    "Use DataFrame’s sample method to view randomly selected rows. **df.sample()** will display one randomly chosen row and **df.sample(n)** will display n randomly chosen rows.\n",
    "## 3.4 Selecting and Filtering Data\n",
    "\n",
    "This chapter demonstrates the basics of selecting and filtering data in Pandas DataFrames. By selecting, we mean choosing a subset (usually columns) of a DataFrame. By filtering, we mean choosing a subset of rows with values that meet certain criteria. Pandas provides many features for selecting and filtering data and there are often multiple ways to do the same thing. We will only scratch the surface of Pandas’ capabilities.\n",
    "### 3.4.1 Selecting Columns of a DataFrame\n",
    "To select a subset of columns, you can call the frame with a list argument inside square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()\n",
    "ratings[['gvkey','datadate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Filtering a DataFrame\n",
    "Filtering is the process of extracting rows of a dataset that meet certain criteria. The main idea behind filtering in Pandas is to pass a Boolean series to a DataFrame; only rows for which the Boolean Series is true are kept in the filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['datadate']=pd.to_datetime(ratings['datadate'],format='%Y-%m-%d')\n",
    "ratings_00s=ratings[(ratings['datadate'].dt.year>=2000) & (ratings['datadate'].dt.year<=2009)].copy()\n",
    "ratings_00s.sort_values('datadate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Creating new columns\n",
    "Each column of a DataFrame is a Series. Therefore, creating a new column implies creating a Series. Typically, new columns are created as transformations of existing columns. However, sometimes new columns contain single values.\n",
    "### 3.5.1 Creating a column from a scalar value\n",
    "Pandas makes it very easy to create a new column that contains a single, repeated value. Simply assign that value to a column of the DataFrame that does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['newcol']=1\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tells Pandas to first create a DataFrame containing a single column. The second line of code references a column named 'newcol' in DataFrame *ratings*. Since that column does not exist, Pandas will create it. When Pandas sees a single, scalar value assigned to a Series, it automatically adds that value to every row of the DataFrame.\n",
    "\n",
    "To summarize, to create a new column in a DataFrame containing\n",
    "a single, repeated value:\n",
    "\n",
    "- Type the DataFrame name followed by square brackets.\n",
    "\n",
    "- Inside the square brackets, type the name of the new column as a string.\n",
    "\n",
    "- After the brackets, type an equals sign and then the desired value.\n",
    "\n",
    "### 3.5.2 Creating a column as a transformation of an existing column\n",
    "Column transformations are very common. Examples of column trans- formation are to (1) strip whitespace from all values in a column, (2) convert a string column to uppercase, and (3) perform a mathematical operation on every value in a column. In this section, we provide sample code that implements these examples. Often, when cleaning data like this, it is desirable to do so “in place” (i.e., instead of creating a new column, replace data in an existing column with transformed data).\n",
    "\n",
    "We will extract the year from 'datadate' in the *ratings* DataFrame and add it to the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['year']=ratings['datadate'].dt.year\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Dropping and renaming columns\n",
    "### 3.6.1 Dropping columns\n",
    "Use the aptly named drop method to drop columns from a Pandas DataFrame. Pass a list of unwanted column names to the columns keyword argument. For example, if we wanted to drop the 'newcol' column from the *ratings* DataFrame, we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.drop(columns=['newcol'], inplace=True)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the keyword argument inplace. By default, the drop method returns a new DataFrame. To force drop to operate in place, this argument is needed. An alternative method for dropping columns in place is to reassign the result to the existing DataFrame. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2=ratings.drop(columns=['year'])\n",
    "ratings2\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Renaming columns\n",
    "The **rename** method works similarly to the **drop** method demonstrated above. However, instead of passing a list to the **columns** keyword argument, rename requires a *dictionary*. Each key in the dictionary must be the name of an existing column; the corresponding value is the new name for that column. This should be clear from the following example.\n",
    "\n",
    "We will change the names of the 'datadate' and 'splticrm' variables in the ratings DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.rename(columns={'datadate':'rating_date','splticrm':'rating'}, inplace=True)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Sorting data\n",
    "To sort data in a DataFrm, use the **sort_values** method. Use the following keyword arguments to achieve the desired result:\n",
    "\n",
    "- **by**: pass a list of column names by which to sort\n",
    "\n",
    "- **ascending**: Use **True** (**False**) to sort all columns in ascending (descending) order. Alternatively, pass a list of Booleans to specify the sort order for each column named in the by argument.\n",
    "\n",
    "- **inplace**: Use **True** to sort in-place. If this argument is omitted or set to **False**, then **sort_values** will return a new, sorted DataFrame.\n",
    "\n",
    "Let's sort the *ratings* DataFrame by year and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ratings=ratings.sort_values(by=['year','rating'], ascending=[True,False])\n",
    "sorted_ratings[(sorted_ratings['year']==2000) & (sorted_ratings['rating'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Summarizing data\n",
    "You can easily use the Pandas package to get summary statistics for your data. Summary statistics can be applied using various methods and can be applied to groups of variables. For instance, we can collapse the EPU data that we imported earlier to the annual level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['year']=df2['date'].dt.year\n",
    "df2\n",
    "## Only retain 'EPU'\n",
    "annual_epu=df2.groupby('year')['epu'].mean()\n",
    "annual_epu\n",
    "\n",
    "## Retain all variables at the annual level\n",
    "annual=df2.groupby('year').mean()\n",
    "annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Merging data\n",
    "Just like in Stata, SAS, and SQL, the Pandas module in Python can merge different DataFrames and Series together in a database-style approach.\n",
    "\n",
    "Let's try to merge the credit rating data with a few variables from the Compustat annual file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=wrds.Connection(wrds_username='sbonsall')\n",
    "\n",
    "comp=db.raw_sql(\"select gvkey, datadate, at from comp.funda where date_part('year',datadate) between 2000 and 2010 and indfmt='INDL' and consol='C' and datafmt='STD'\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp['datadate']=pd.to_datetime(comp['datadate'],format='%Y-%m-%d')\n",
    "comp.info()\n",
    "ratings.info()\n",
    "merged=pd.merge(comp,ratings, left_on=['gvkey','datadate'], right_on=['gvkey','rating_date'],indicator=True, validate='1:1')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Visualization\n",
    "You can do a lot with graphs in Pandas and Python. The main module for plotting is called **matplotlib**. We will load that and then see how to graph. Obviously, you will have to play around with plotting for a while to a hang of all the available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.info()\n",
    "df.plot(x='Date')\n",
    "plt.ylabel('Index Level')\n",
    "plt.legend(loc='right', bbox_to_anchor=(1.7, 0.5))\n",
    "plt.title('Policy Uncertainty Over Time')\n",
    "plt.savefig('epu.png',dpi=400, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
