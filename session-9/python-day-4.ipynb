{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Extracting data from the web\n",
    "We have, thus far, focused on some of the Python fundamentals. Today, we will put some of that to work and build a script that will scrape the NYU Law School Securities Enforcement Empirical Database. This database houses a significant amount of information about SEC enforcements against individuals and companies.\n",
    "## 1 How do we pull in web content?\n",
    "There are several ways to import web content into Python strings or lists, but probably the easiest to use is the **requests** library. You will need to import this library from **pip** before executing any commands in this lesson.\n",
    "### 1.1 Making a request\n",
    "The first task involved with making a request is assigning a URL to a Python object. Let's try this with my faculty page on the Smeal website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "page=requests.get(\"https://directory.smeal.psu.edu/sbb151\", verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see whether the web page loaded without error, we can call the **status_code** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A status code of \"200\" means everything is okay. You have probably seen the staus code \"403 Forbidden\" from a website at times. That is another possible code you could get. If you get anything but \"200\", you will need to investigate further. Error handling comes in handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if page.status_code==200:\n",
    "    print(\"Everything is good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The content of the page\n",
    "You can extract the content of the page by using the **text** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Making sense of the HTML content\n",
    "As you can see from my Smeal directory web page above, extracting the actual text of the HTML code is only step one of the process for scraping web content. We need some way to \"look\" through the code and find what we are interested in collecting. Fortunately, Python has the **BeautifulSoup** module for parsing the HTML code. Let's dif into **BeautifulSoup**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read the HTML content into a BeautifulSoup object\n",
    "We can read the text of the website into a BeautifulSoup object but also should specifiy the parser. I often use the \"html5lib\" because it is very lenient in interpreting the HTML code that it sees. The \"html5lib\" parser claims to parse pages the same way a web browser does, which is quite useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page.text,'html5lib')\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like out \"soup\" object contains the same information as when we simply printed \"page.text\". However, BeautifulSoup has combed through the code and given use the ability to search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tags\n",
    "BeautifulSoup captures the HTML tags for every tag in the code. You can access them as attributes of the main BeautifulSoup object. For instance, let's print the first \"link\" tag. Those looks like &lt;a&gt; in HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to capture all of a given tag, you can easily loop through the tags using a **for** loop and the **find_all** method in BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=soup.find_all('a')\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few links in my directory page! What if we want to extract my bio entry? How would we do that? Well, we will need to scan the code to find the biography and see if there are any tags that might be of interest.\n",
    "\n",
    "From my inspection, the biography is housed in a &lt;div&gt; tag with the additional attrribute of id=\"biography\". That should be easy to extract with both of those identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.find(id='biography'))\n",
    "print(soup.find(id='biography').p.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right? Let's turn our attention to getting some lawsuit data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Extracting SEC enforcement data\n",
    "### 3.1 Import some Python modules\n",
    "We are going to need some Python modules to help us out. Let's go ahead and import them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from math import ceil\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Looking at the SEED data\n",
    "Let's take a look at the [filings](https://research.seed.law.nyu.edu/Search/Results) page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"https://research.seed.law.nyu.edu/Search/Results\"\n",
    "page = requests.get(seed)\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data we want to extract is housed in an HTML table. We can use BeautifulSoup to extract elements of the table and build a Pandas dataframe that we can use for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html5lib')\n",
    "table = soup.find('table')\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract 3.2 SEED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. We will extract the body of the table with the results\n",
    "\n",
    "tbody=table.find('tbody')\n",
    "\n",
    "## 2. Next, we will loop through each row and save the data into a dictionary\n",
    "data=[]\n",
    "for i, row in enumerate(tbody.find_all('tr')):\n",
    "    rdata=[]\n",
    "    if i==0:\n",
    "        for cell in row.find_all('th'):\n",
    "            rdata.append(cell.a.get_text())\n",
    "    else:\n",
    "        for cell in row.find_all('td'):\n",
    "            rdata.append(cell.get_text().strip())\n",
    "    data.append(rdata)\n",
    "\n",
    "print(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Convert to a Pandas dataframe\n",
    "Let's now take our list of data and convert it into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(data[1:], columns=data[0])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data to Excel\n",
    "\n",
    "Now, we will export our dataframe to Excel so that we can use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('SEED_data.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
